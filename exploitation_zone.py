from email.utils import decode_rfc2231
import os
from sys import path
import time
from pyspark import Row

from pyspark.sql import SparkSession
from pyspark.sql.functions import to_json, input_file_name, current_timestamp, avg, count
from pyspark.sql.types import BooleanType, StringType, NumericType, FloatType

from models.datasource import Datasource
from models.format_log_entry import FormatLogEntry

from process import Process
from utils.properties_parser import parse_properties


class ExploitationZoneProcess(Process):
    _log_collection_name = 'formatLog'

    def __init__(self):
        self.properties_db = {
            "user": parse_properties('monetdb')['database.user'],
            "password": parse_properties('monetdb')['database.password'],
            "driver": "org.monetdb.jdbc.MonetDriver",
            "batchsize": 10000
        }
        super().__init__()
    
    def run_process(self):
        self._top_listings_per_neighborhood()
        self._correlation_average_price_index_per_neighborhood()
        self._correlation_price_index_per_neighborhood()
        self._neighborhood_commerce_categories()
    
    def _top_listings_per_neighborhood(self):
        spark = SparkSession.builder.master("local[*]").appName("top_listings_per_neighborhood").config('spark.driver.extraClassPath',
                './drivers/monetdb-jdbc-3.2.jre8.jar').getOrCreate()
        df = spark.read.format('jdbc').options(url=f"jdbc:monetdb://{parse_properties('monetdb')['database.host']}:50000/mydb", dbtable='idealista', **self.properties_db).load()
        
        df = df.select('district_n_reconciled', 'district_id', 'neighborhood_n_reconciled', 'neighborhood_id')
        
        # Conver to RDD just to learn
        rdd = df.rdd.map(lambda entry: (entry.neighborhood_id, (entry.district_id, entry.district_n_reconciled, entry.neighborhood_n_reconciled, 1)))
        rdd = rdd.filter(lambda entry: entry[0] is not None)
        rdd = rdd.reduceByKey(lambda x, y: (x[0], x[1], x[2], x[3]+y[3]))

        # Converto to Dataframe to store to JDBC
        rdd = rdd.map(lambda entry: Row(neighborhood_id=entry[0], district_id=entry[1][0], district_n_reconciled=entry[1][1], 
                neighborhood_n_reconciled=entry[1][2], count_list=entry[1][3]))

        # To dataframe to store to JDBC
        
        df = rdd.toDF()
        self._write_to_monet(df, 'idealista_count_neighbors')
        
    
    def _correlation_average_price_index_per_neighborhood(self):
        spark = SparkSession.builder.master("local[*]").appName("top_listings_per_neighborhood").config('spark.driver.extraClassPath',
                './drivers/monetdb-jdbc-3.2.jre8.jar').getOrCreate()
        df_idealista = spark.read.format('jdbc').options(url=f"jdbc:monetdb://{parse_properties('monetdb')['database.host']}:50000/mydb", dbtable='idealista', **self.properties_db).load()
        df_opendata_inc = spark.read.format('jdbc').options(url=f"jdbc:monetdb://{parse_properties('monetdb')['database.host']}:50000/mydb", dbtable='opendatabcn_income', **self.properties_db).load()
        
        df_opendata_inc = df_opendata_inc.select("neighborhood_id", "index_rfid")

        df = df_idealista.join(df_opendata_inc, ['neighborhood_id'])
        df = df.groupBy("neighborhood_id", "neighborhood_n_reconciled", "district_id", "district_n_reconciled")\
                .agg(avg("index_rfid").alias("avg_index"), avg("price").alias("avg_price"))

        df = df.select('avg_index', 'avg_price', 'neighborhood_id' ,'neighborhood_n_reconciled', 'district_id', 'district_n_reconciled')

        self._write_to_monet(df, 'correlation_average_price_index_neigh')
        
        
    def _correlation_price_index_per_neighborhood(self):
        spark = SparkSession.builder.master("local[*]").appName("top_listings_per_neighborhood").config('spark.driver.extraClassPath',
                './drivers/monetdb-jdbc-3.2.jre8.jar').getOrCreate()
        df_idealista = spark.read.format('jdbc').options(url=f"jdbc:monetdb://{parse_properties('monetdb')['database.host']}:50000/mydb", dbtable='idealista', **self.properties_db).load()
        df_opendata_inc = spark.read.format('jdbc').options(url=f"jdbc:monetdb://{parse_properties('monetdb')['database.host']}:50000/mydb", dbtable='opendatabcn_income', **self.properties_db).load()
        
        df_opendata_inc = df_opendata_inc.select("neighborhood_id", "index_rfid")
        df_opendata_inc = df_opendata_inc.groupBy("neighborhood_id").agg(avg("index_rfid").alias("avg_index"))

        df = df_idealista.join(df_opendata_inc, ['neighborhood_id'])
        # df = df.groupBy("neighborhood_id", "neighborhood_n_reconciled", "district_id", "district_n_reconciled")\
        #        .agg(avg("index_rfid").alias("avg_index"), avg("price").alias("avg_price"))

        df = df.select('avg_index', 'price', 'neighborhood_id' ,'neighborhood_n_reconciled', 'district_id', 'district_n_reconciled')
        df = df.withColumn("avg_index", df["avg_index"].cast(FloatType()))

                # To dataframe to store to JDBC
        self._write_to_monet(df, 'correlation_price_index_neigh')


    def _neighborhood_commerce_categories(self):
        spark = SparkSession.builder.master("local[*]").appName("top_listings_per_neighborhood").config('spark.driver.extraClassPath',
                './drivers/monetdb-jdbc-3.2.jre8.jar').getOrCreate()
        df_idealista = spark.read.format('jdbc').options(url=f"jdbc:monetdb://{parse_properties('monetdb')['database.host']}:50000/mydb", dbtable='idealista', **self.properties_db).load()
        df_opendata_commerce = spark.read.format('jdbc').options(url=f"jdbc:monetdb://{parse_properties('monetdb')['database.host']}:50000/mydb", dbtable='opendatabcn_commercial', **self.properties_db).load()
        
        df_opendata_commerce = df_opendata_commerce.select("neighborhood_id", "Codi_Grup_Activitat", "Nom_Grup_Activitat")


        df = df_idealista.join(df_opendata_commerce, ['neighborhood_id'])
        df = df.groupBy("neighborhood_id", "Codi_Grup_Activitat", "Nom_Grup_Activitat", "neighborhood_n_reconciled", "district_id", "district_n_reconciled")\
                .agg(count("*").alias("n_commerces"))
        
        df = df.select('neighborhood_id', 'Codi_Grup_Activitat', 'Nom_Grup_Activitat', 'n_commerces', 'neighborhood_n_reconciled', 'district_id', 'district_n_reconciled')

        # To dataframe to store to JDBC
        self._write_to_monet(df, 'count_commerce_categories')

    
    def _write_to_monet(self, df, table):
        # To dataframe to store to JDBC
        columns_modified = []
        
        string_columns = [x.name + " VARCHAR(1024)" for x in df.schema.fields if isinstance(x.dataType, StringType)]
        columns_modified.extend(string_columns)
        column_types = ', '.join(columns_modified) if len(columns_modified) > 0 else ''
        # Write to SQL
        write_command = df.write
        
        if column_types != '':
            write_command = write_command.option("createTableColumnTypes", column_types)
        
        write_command.format("jdbc").mode('overwrite').options(url=f"jdbc:monetdb://{parse_properties('monetdb')['database.host']}:50000/mydb", 
            dbtable=table, **self.properties_db).save()


    
