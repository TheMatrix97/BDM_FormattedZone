{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54691ae6",
   "metadata": {},
   "source": [
    "# Applying a Spark MLlib Model to a Kafka Data Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f791eba",
   "metadata": {},
   "source": [
    "Step 1: Connect to UPC VPN since the Kafka stream is from a UPC server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b66405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pyspark.ml import PipelineModel\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c73451",
   "metadata": {},
   "source": [
    "Structured Streaming Programming Guide:\n",
    "- https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "RDD Programming Guide: \n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "Spark Streaming Programming Guide:\n",
    "- https://spark.apache.org/docs/latest/streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c0a149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/21 20:48:25 WARN Utils: Your hostname, Kathryns-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.37 instead (on interface en0)\n",
      "22/06/21 20:48:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/kat/opt/anaconda3/envs/bdm/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/kat/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kat/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ffbb12da-6c0c-4418-8f7e-3a4e86451fb8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in local-m2-cache\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in local-m2-cache\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1098ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from local-m2-cache in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ffbb12da-6c0c-4418-8f7e-3a4e86451fb8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/23ms)\n",
      "22/06/21 20:48:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# initializing spark session\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"StreamPredictor\").config('spark.driver.extraClassPath','./drivers/monetdb-jdbc-3.2.jre8.jar').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1372fc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# properties to connect to kafka stream\n",
    "server = 'venomoth.fib.upc.edu:9092'\n",
    "topic = 'bdm_p2'\n",
    "consumer = KafkaConsumer(topic, bootstrap_servers=server)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9692a6",
   "metadata": {},
   "source": [
    "References: \n",
    "- https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/\n",
    "- https://spark.apache.org/docs/3.3.0/api/python/reference/api/pyspark.ml.PipelineModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44101f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loads previously saved model from the notebook MachineLearining.ipynb\n",
    "model = PipelineModel.load('Fitted-ML-Pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53fb5b",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\n",
    "\n",
    "The streaming dataframe needs to use the same variable names as used in the Machine Learning Pipeline Model: \"neighborhood_id\" & \"price\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f64c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe of kafka stream data\n",
    "df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", server).option(\"subscribe\", topic).load()\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "# https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\n",
    "split_col = split(df['value'], ',')\n",
    "df = df.withColumn('date', split_col.getItem(0))\n",
    "df = df.withColumn('neighborhood_id', split_col.getItem(1))\n",
    "df = df.withColumn('price', split_col.getItem(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba92a9c",
   "metadata": {},
   "source": [
    "Pim & Enric provided some guidance with the following code in order to make the predictions and save them to a dataframe in memory for periodic querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db88a85b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating predictions\n",
    "predict = model.transform(df).select(\"date\",\"neighborhood_id\", \"price\", \"prediction\")\n",
    "# adding column to calculate error\n",
    "predict = predict.withColumn('error', (predict['price']-predict['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b0fea",
   "metadata": {},
   "source": [
    "There are different options for displaying the predictions. We can print them to console in real time or save them to a dataframe and query it periodically. Both options work, but I have chosen to print them to console in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72cc2ef4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/21 20:48:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/fm/mghr685s1l70_6sxmqyh0zlc0000gn/T/temporary-c36de88a-6026-4d97-a2eb-8e93747341ff. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/21 20:48:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+---------------+-----+----------+-----+\n",
      "|date|neighborhood_id|price|prediction|error|\n",
      "+----+---------------+-----+----------+-----+\n",
      "+----+---------------+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+-------+------------------+-----------------+\n",
      "|                date|neighborhood_id|  price|        prediction|            error|\n",
      "+--------------------+---------------+-------+------------------+-----------------+\n",
      "|2022-06-21 18:48:...|       Q1904302|3479966|1121793.0438250077|2358172.956174992|\n",
      "+--------------------+---------------+-------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "|                date|neighborhood_id| price|       prediction|            error|\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "|2022-06-21 18:48:...|       Q1026658|560034|502431.0746205747|57602.92537942529|\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "|                date|neighborhood_id| price|       prediction|             error|\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "|2022-06-21 18:48:...|       Q3298510|245007|312176.4716073198|-67169.47160731978|\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+------------------+-------------------+\n",
      "|                date|neighborhood_id| price|        prediction|              error|\n",
      "+--------------------+---------------+------+------------------+-------------------+\n",
      "|2022-06-21 18:48:...|        Q720994|350027|512946.81859019975|-162919.81859019975|\n",
      "+--------------------+---------------+------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "|                date|neighborhood_id| price|        prediction|             error|\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "|2022-06-21 18:48:...|       Q3753110|289034|277663.04870485375|11370.951295146253|\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "|                date|neighborhood_id| price|       prediction|             error|\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "|2022-06-21 18:48:...|       Q3291762|449963|732745.2440426771|-282782.2440426771|\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "|                date|neighborhood_id| price|        prediction|             error|\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "|2022-06-21 18:48:...|       Q3753110|268928|277663.04870485375|-8735.048704853747|\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "|                date|neighborhood_id| price|       prediction|            error|\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "|2022-06-21 18:48:...|       Q3750859|765099|384644.9921087892|380454.0078912108|\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "|                date|neighborhood_id| price|        prediction|             error|\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "|2022-06-21 18:48:...|       Q3321657|950057|1688292.4283983978|-738235.4283983978|\n",
      "+--------------------+---------------+------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "|                date|neighborhood_id| price|       prediction|            error|\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "|2022-06-21 18:49:...|       Q1026658|749901|502431.0746205747|247469.9253794253|\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+-------------------+\n",
      "|                date|neighborhood_id| price|       prediction|              error|\n",
      "+--------------------+---------------+------+-----------------+-------------------+\n",
      "|2022-06-21 18:49:...|       Q3296693|449918|571935.8509607569|-122017.85096075688|\n",
      "+--------------------+---------------+------+-----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+----------------+\n",
      "|                date|neighborhood_id| price|       prediction|           error|\n",
      "+--------------------+---------------+------+-----------------+----------------+\n",
      "|2022-06-21 18:49:...|       Q3320705|239949|235611.1218030587|4337.87819694131|\n",
      "+--------------------+---------------+------+-----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "|                date|neighborhood_id| price|       prediction|             error|\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "|2022-06-21 18:49:...|        Q980253|249950|288158.1964944844|-38208.19649448438|\n",
      "+--------------------+---------------+------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "|                date|neighborhood_id| price|       prediction|            error|\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "|2022-06-21 18:49:...|       Q1026658|599936|502431.0746205747|97504.92537942529|\n",
      "+--------------------+---------------+------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kat/opt/anaconda3/envs/bdm/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/kat/opt/anaconda3/envs/bdm/lib/python3.9/site-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/kat/opt/anaconda3/envs/bdm/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start streaming the data and predictions to the console\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m predict\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bdm/lib/python3.9/site-packages/pyspark/sql/streaming.py:101\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bdm/lib/python3.9/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bdm/lib/python3.9/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bdm/lib/python3.9/site-packages/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bdm/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start streaming the data and predictions to the console\n",
    "query = predict.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777bf39",
   "metadata": {},
   "source": [
    "To save predictions to a dataframe in memory, comment out the two code chunks above, and uncomment the three code chunks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18aba635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/21 20:49:15 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@b33eea1 is aborting.\n",
      "22/06/21 20:49:15 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@b33eea1 aborted.\n",
      "22/06/21 20:49:16 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/21 20:49:16 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 31, attempt 0, stage 31.0)\n",
      "22/06/21 20:49:16 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 31, attempt 0, stage 31.0)\n",
      "22/06/21 20:49:16 WARN TaskSetManager: Lost task 0.0 in stage 31.0 (TID 31) (192.168.1.37 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save streaming data and predictions into the dataframe in memory.\n",
    "# predict.writeStream.queryName(\"predict\").format(\"memory\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b1bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## query the \"predict\" dataframe after a few seconds to see results\n",
    "# spark.sql(\"select * from predict\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca600e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f01825",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bdm]",
   "language": "python",
   "name": "conda-env-bdm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
